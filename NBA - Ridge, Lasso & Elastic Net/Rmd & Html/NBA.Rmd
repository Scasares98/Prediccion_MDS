---
title: "Nba - Modelo Ridge, Lasso y Elastic Net"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```
# 1.Introducción al trabajo

El próximo documento, se tratará de encontrar el modelo predictivo que mayor capacidad de predicción tenga acerca de los salarios de los jugadores de la NBA en función de otras 29 variables.

El dataset consta, tras eliminar los valores nulos, de 485 observaciones y 28 variables, las cuales se resumirán en las siguientes líneas del documento:

1:Player           Nombre del jugador.

2:Salary          Salario del jugador.

3:NBA_Country      Nacionalidad del jugador.

4:NBA_DraftNumber  Número del draft.

5:Age              Edad del jugador.

6:Tm              Abreviatura del equipo

7:G               Partidos jugados

8:MP              Minutos jugados

9:PER             Eficiencia de jugador

10:TS%            Porcentaje de tiro

11:3PAr           % de triples

12:FTr             % de tiros libres

13:ORB%           % Rebotes Ofensivos ganados

14:DRB%           % Rebotes defensivos ganados

15:TRB%            % Rebotes totales

16:AST%           % Asistencia

17:STL%           % Robos

18:BLK%            % Bloqueos

19:TOV%           % Robo previo a tiro

20:USG%           % de participacion en jugadas

21:OWS            Acciones en ataque acertadas

22:DWS             Acciones defensivas acertadas

23:WS             Victorias contribuidas

24:WS/48           Ratio de contribución por partido

25:OBPM          +/- rendimiento respecto al equipo (cada 100 jugadas en ataque)

26:DBPM            +/- rendimiento respecto al equipo (cada 100 jugadas en defensa)

27:BPM             +/- rendimiento respecto al equipo (cada 100 posesiones generales)

28:VORP            Valor respecto a jugador involucrado en el cambio

# 2.Importación de librerías y Dataset

```{r}
library(glmnet)
library(tidyverse)
library(fBasics)
library(car)
library(dplyr)
library(ggplot2)
library(knitr)
library(MASS)
library(corrplot)
library(PerformanceAnalytics)
library(gvlma)
library(tinytex)
library(devtools)
```


```{r}
nba <- read_delim("nba_2.csv", ";", escape_double = FALSE, 
                    trim_ws = TRUE)

attach(nba)
names(nba)
head(nba)
summary(nba)
```

# 3. Limpieza del Dataset

```{r}
#Limpieza de datos
nba <- na.omit(nba)

#Convertir la variable Tm (Teams) de cualitativa a Cuantitativa
unique(nba$Tm)
levels <- c('HOU', 'GSW', 'SAC', 'CHI', 'POR',
            'DAL', 'BOS', 'MEM', 'DEN',
            'TOT', 'LAC', 'ORL', 'MIA',
            'IND', 'LAL', 'MIN', 'PHO',
            'ATL', 'CLE', 'NYK', 'CHO',
            'MIL', 'SAS', 'UTA', 'NOP',
            'WAS', 'PHI', 'BRK', 'OKC',
            'DET', 'TOR')

nba$Team <- match(nba$Tm, levels)

#Convertir la variable NBA_Country (Países) de cualitativa a Cuantitativa
unique(nba$NBA_Country)
niveles <- c('China', 'Georgia', 'USA', 'Canada', 'Spain',
            'France', 'Czech Republic', 'Russia', 'South Sudan',
            'Switzerland', 'New Zealand', 'Haiti', 'Democratic Re_',
            'Tunisia', 'Brazil', 'Germany', 'Australia',
            'Cameroon', 'Israel', 'Turkey', 'United Kingdo...',
            'Montenegro', 'Serbia', 'Argentina', 'Bosnia',
            'Lithuania', 'Croatia', 'Italy', 'Poland',
            'Dominican Rep...', 'Finland', 'Latvia', 'Bosnia & Herz...',
            'Sweden', 'Ukraine', 'Austria', 'Puerto Rico',
            'Senegal', 'Slovenia', 'Greece', 'Democratic Re...',
            'Mali', 'Bahamas', 'Egypt')
nba$Pais <- match(nba$NBA_Country, niveles)
```

# 4. Ejemplificación Modelo Lineal con todas las variables

```{r}
#Realizamos el modelo con todas las variables numericas y las cualitativas convertidas a cuantitativas

model <- lm(log(Salary, base = 10) ~ NBA_DraftNumber + Age + G + MP + PER + PAr + FTr + ORB + DRB + TRB +
              AST + STL + BLK + TOV + USG + OWS + DWS + WS + WS_1 + OBPM + DBPM + BPM + VORP + Team + Pais, data = nba)
model
```

# 5. División dataset Training vs Testing

Dividimos el datase de nba en dos parte, una, que representa el 70% de los datos, para entrenar a los modelos y luego otra, que representa el 30% que servirá para poder testear la eficacia del modelo.

```{r}
library(rsample)

set.seed(2020)

#Dividir el Dataset

nba_split <- initial_split(nba, prop = .7)

dim(nba)
#Practica (70%)
nba_training_data <- training(nba_split) 
dim(nba_training_data)
#Test (30%)
nba_testing_data <- testing(nba_split) 
dim(nba_testing_data)

# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept
nba_training_data_a <- model.matrix(Player ~ ., nba_training_data)[, -1]
nba_training_data_b <- log(nba_training_data$Salary)

nba_testing_data_a <- model.matrix(Player ~ ., nba_testing_data)[, -1]
nba_testing_data_b <- log(nba_testing_data$Salary)
```

# 6. Regresiones

## 6.1. Ridge Regression

A continuación, vamos a realizar la Regresión de Ridge, esta regresión busca estimadores de coecifientes que reducen el RSS, pero penaliza aquellos coecifientes que se acerquen a cero, por eso, esta estimación tiene el efecto de reducir las estimacions del coecifiente a cero.

En este apartado, seguiremos la siguiente estructura:

- Visualización de las estimaciones de los coecifientes de las variables con la función glmnet.

- Visualización de los MSE (Mean Squared Error) para cada uno de los valores de Lambda.

- Elegir el modelo con menor valor de MSE y sumarle una desviación típica a la derecha (debido a que se favorece a los modelos parsimoniosos, es decir, más sencillos).

- Visualización del valor de LOG(Lambda) escogido junto con el valor de las estimaciones de los coecifientes.

- Visualización de la tabla con el top 25 variables que más influyen en el modelo de Ridge.

```{r}
nba_ridge <- glmnet(
  x = nba_training_data_a,
  y = nba_training_data_b,
  alpha = 0
)

plot(nba_ridge, xvar = 'lambda', main = 'Ridge')

#Encabezado
nba_ridge$lambda %>% head()
names(nba_ridge)


#------------
#Tuning A - Ver modelo con menor error al cuadrado en funcion de su lambda
#------------

nba_ridge_cv <- cv.glmnet(
  x = nba_training_data_a,
  y = nba_training_data_b,
  alpha = 0
)

# plot results
plot(nba_ridge_cv)


#Minimo
min(nba_ridge_cv$cvm) #0.6679
#Valor de lambda para el minimo
nba_ridge_cv$lambda.min  #0.112
log(nba_ridge_cv$lambda.min) #-2.185

#Minimo LAMBDA + 1 desv estandar
nba_ridge_cv$cvm[nba_ridge_cv$lambda == nba_ridge_cv$lambda.1se]
nba_ridge_cv$lambda.1se
#Valor Lmabda
log(nba_ridge_cv$lambda.1se)

#Grafica del valor de los coecifientes para el valor de lambda
plot(nba_ridge, xvar = "lambda")
abline(v = log(nba_ridge_cv$lambda.1se), col = "red", lty = "dashed")

#Tabla top 25 coecifientes con mas influencia
coef(nba_ridge_cv, s = "lambda.1se") %>%
  broom::tidy() %>%
  top_n(25, wt = abs(value)) %>%
  ggplot(aes(value, reorder(row, value))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```


Podemos observar que para este modelo, el valor de lambda que genera un menor RSE es cuando Log(Lambda) es igual a -0.51 (aunque este puede variar en función de la muestra).

También se muestra el top 25 variables que más afectan al modelo y con un mayor valor de coecifientes (sin tener en cuenta 'Intercept').


## 6.2 Lasso Regression

La Regresión Lasso es un método de análisis de regresión que realiza selección de variables y regularización para mejorar la exactitud e interpretabilidad del modelo estadístico producido por este.

La penalización de la Regresión Lasso se da por el valor absoluto de los parámetro, lo que provoca que según Log(Lambda) se hace más estricto, es decir, aumenta su valor positivamente, se reduce el número de variables que conforman el modelo.

En este apartado, seguiremos la siguiente estructura:

- Visualización de las estimaciones de los coecifientes de las variables con la función glmnet.

- Visualización de los MSE (Mean Squared Error) para cada uno de los valores de Lambda.

- Elegir el modelo con menor valor de MSE y sumarle una desviación típica a la derecha (debido a que se favorece a los modelos parsimoniosos, es decir, más sencillos).

- Visualización del valor de LOG(Lambda) escogido junto con el valor de las estimaciones de los coecifientes.

- Visualización de la tabla con las variables que más influyen en el modelo de Lasso

```{r}
#Calculo coecifientes en funcion de lambda
nba_lasso <- glmnet(
  x = nba_training_data_a,
  y = nba_training_data_b,
  alpha = 1
)

plot(nba_lasso, xvar = "lambda")


#Calculo medio de residuos en funcion del logaritmo de lambda

nba_lasso_cv <- cv.glmnet(
  x = nba_training_data_a,
  y = nba_training_data_b,
  alpha = 1
)

plot(nba_lasso_cv)

#Valor de lambda para el minimo MSE
min(nba_lasso_cv$cvm) 
#Valor minimo de lambda
nba_lasso_cv$lambda.min

#Valor minimo de lambda + 1 desviacion estandar
nba_lasso_cv$cvm[nba_lasso_cv$lambda == nba_lasso_cv$lambda.1se]
log(nba_lasso_cv$lambda.1se)


#Grafica del valor de los coecifientes en funcion de lambda

plot(nba_lasso, xvar = 'lambda')
abline(v = log(nba_lasso_cv$lambda.min), col = "red", lty = "dashed")
abline(v = log(nba_lasso_cv$lambda.1se), col = "red", lty = "dashed")


#Tabla coecifientes con mas influencia


coef(nba_lasso_cv, s = "lambda.1se") %>%
  tidy() %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

Podemos observar que para este modelo, el valor de lambda que genera un menor RSE es cuando Log(Lambda) es igual a -1.65 (aunque este puede variar en función de la muestra).

También se muestran las variables que más afectan al modelo y con un mayor valor de coecifientes (sin tener en cuenta 'Intercept').


## 6.3. Elastic Net Regression

La Regresión de Elastic Net no es más que la combinación lineal de ambos modelos.

La Regresión de Lasso y Ridge no son más que dos casos particulares de Elastic Net donde Alpha = 1 (Lasso) o Alpha = 0 (Ridge).

Elastic Net ajusta el valor de alpha para realizar la combiación de Lasso y Ridge.

La estructura que se seguirá en este apartado es:

- Visualización de los estimadores de los coecifientes para diferentes valores de Lambda (0, 0.25, 0.75 y 1)

- Visualización de los MSE (minimos valores de errores al cuadrado), para cada uno de los valores de alpha (de 0 a 1 en intervalos de 0.1)

```{r}
lasso    <- glmnet(nba_training_data_a, nba_training_data_b, alpha = 1.0) 
elastic1 <- glmnet(nba_training_data_a, nba_training_data_b, alpha = 0.25) 
elastic2 <- glmnet(nba_training_data_a, nba_training_data_b, alpha = 0.75) 
ridge    <- glmnet(nba_training_data_a, nba_training_data_b, alpha = 0.0)

par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")


#Ajuste del modelo (tuning)

# maintain the same folds across all models
fold_id <- sample(1:10, size = length(nba_training_data_b), replace=TRUE)

# search across a range of alphas
tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)
tuning_grid


for(i in seq_along(tuning_grid$alpha)) {
  
  # fit CV model for each alpha value
  fit <- cv.glmnet(nba_training_data_a, nba_training_data_b, alpha = tuning_grid$alpha[i])
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

tuning_grid


tuning_grid %>%
  mutate(se = mse_1se - mse_min) %>%
  ggplot(aes(alpha, mse_min)) +
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
  ggtitle("MSE ± one standard error")

```

Independientemente de la muestra que se haya escogido, se puede observar que el error es más elevado para los valores de alpha entre 0 y 0.25 ó 0.30, es por eso que cualquier modelo entre alpha 0.30 hasta 1 nos puede servir para poder predecir los salarios de la NBA.

Pero debido a nuestro sesgo de querer elegir modelos parsimoniosos, es decir, más sencillos, el modelo que más se ajsuta a nuestras necesidades es el modelo Lasso (alpha = 1), cuyo mse es de 0.575 aproximadamente.



# 7. Predicción

En este apartado, auqnue ya hayamos observado que el modelo Lasso es que mejor predice, vamos a ponerle valor a esa predicción, para ello, utilizaremos ese 30% de los datos que no habiamos utilizado para poder ver cuál es el error medio de la predicción de los valores.

Se obtiene que el error cuadrado mínimo es de 0.556, pero el error medio de predicción no puedo calcularlo por errores que no he podido solucionar.

```{r}
#Lasso
cv_lasso <- cv.glmnet(nba_training_data_a, nba_training_data_b, alpha = 1.0)
min(cv_lasso$cvm)

```


